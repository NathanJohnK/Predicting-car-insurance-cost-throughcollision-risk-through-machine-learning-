{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #set the size of our plots as they are a little small by default.\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#mac path\n",
    "mac_file_path = r'/Users/nathanstuttard/Portfolio/Collision_trips_ML_Project/Predicting-car-insurance-cost-throughcollision-risk-through-machine-learning-/Data/LBD_New_York_collisions_and_weather_data.csv'\n",
    "\n",
    "#windows path\n",
    "Ubuntu_file_path = r'/home/user/Data Science Projects/Taxi/Predicting-car-insurance-cost-throughcollision-risk-through-machine-learning-/Data/LBD_New_York_collisions_and_weather_data.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(mac_file_path):\n",
    "    df = pd.read_csv(mac_file_path)  # Read the CSV into a DataFrame\n",
    "    print(df.head())  # Display the first few rows of the DataFrame\n",
    "else:\n",
    "    print(\"File not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"year\", \"mo\", \"da\"], ascending = (True, True, True)) # order the data by year, month, day in ascending order.\n",
    "df.head() # check the data again by viewing the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_collisions = df['NUM_COLLISIONS'].sum()\n",
    "print(type(total_collisions))\n",
    "print(\"Total collisions before cleaning:\", total_collisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original df\n",
    "original_df = df\n",
    "\n",
    "df_cleaned = df[(df[\"NUM_COLLISIONS\"] > 350) & (df[\"NUM_COLLISIONS\"] < 900)]\n",
    "df_cleaned = df_cleaned.dropna(how='all')\n",
    "df_cleaned.dropna(subset=['NUM_COLLISIONS'], inplace=True) \n",
    "df_cleaned['collision_date'] = pd.to_numeric(df['collision_date'], errors='coerce')  \n",
    "total_collisions_post_clean = df_cleaned['NUM_COLLISIONS'].sum()\n",
    "print(\"Total collisions before cleaning:\", total_collisions)\n",
    "print(\"Total total_collisions_post_clean:\", total_collisions_post_clean)\n",
    "difference = total_collisions - total_collisions_post_clean\n",
    "print(f\"Dropped {difference} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision_grouping_decorator(group_by):\n",
    "    \"\"\"Decorator to group collisions by day or month.\"\"\"\n",
    "    def wrapper(func):\n",
    "        def inner(dataframe):\n",
    "            collisions = dataframe.groupby(group_by)['NUM_COLLISIONS'].sum().reset_index()\n",
    "            collisions.columns = ['Group', 'Total Collisions']\n",
    "            return collisions.sort_values(by='Total Collisions', ascending=False)\n",
    "        return inner\n",
    "    return wrapper\n",
    "\n",
    "@collision_grouping_decorator('day')\n",
    "def group_collisions_by_day(dataframe):\n",
    "    \"\"\"Groups collisions by day of the week.\"\"\"\n",
    "    pass  # The actual logic is handled by the decorator\n",
    "\n",
    "@collision_grouping_decorator('mo')\n",
    "def group_collisions_by_month(dataframe):\n",
    "    \"\"\"Groups collisions by month.\"\"\"\n",
    "    pass  # The actual logic is handled by the decorator\n",
    "\n",
    "# Get collisions grouped by day and month\n",
    "collisions_per_day_df = group_collisions_by_day(df_cleaned)\n",
    "collisions_per_month_df = group_collisions_by_month(df_cleaned)\n",
    "\n",
    "# Print results\n",
    "print(\"Collisions per Day:\")\n",
    "print(collisions_per_day_df)\n",
    "\n",
    "print(\"\\nCollisions per Month:\")\n",
    "print(collisions_per_month_df)\n",
    "\n",
    "\n",
    "##Interesting - Friday as the most collisions, with Sunday the least - rush hour, commuting\n",
    "## so what's the connection between Friday and Sunday. Sunday is a quieter day, Friday busier. Don't have a time of day so can't look at this in more detail.\n",
    "##is there time data available?\n",
    "\n",
    "\n",
    "#Correlation between weather? Rainfall on a particular day? or by month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine weather data etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'year' and summing 'NUM_COLLISIONS' for each year\n",
    "collisions_per_year = df_cleaned.groupby('year')['NUM_COLLISIONS'].sum()\n",
    "\n",
    "# Convert the series to a DataFrame for plotting\n",
    "collisions_per_year_df = collisions_per_year.reset_index()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(collisions_per_year_df['year'], collisions_per_year_df['NUM_COLLISIONS'], color='skyblue')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.title('Number of Collisions Per Year')\n",
    "plt.xticks(collisions_per_year_df['year'])  # Ensure all years are displayed on the x-axis\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Narrative - ok clearly collisions are increasing, year on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Narrative - I am going to examine whether the number of collisions for each day of the week changes from year to year\n",
    "\n",
    "# Define a mapping for days of the week\n",
    "day_mapping = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', 4: 'Thursday', 5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
    "\n",
    "# Apply the day mapping to the 'day' column\n",
    "df_cleaned.loc[:, 'day_mapped'] = df_cleaned['day'].map(day_mapping)\n",
    "\n",
    "# Group by year and day of the week, then sum the number of collisions\n",
    "collisions_per_day_per_year = df_cleaned.groupby(['year', 'day_mapped'])['NUM_COLLISIONS'].sum().reset_index()\n",
    "\n",
    "# Pivot the table to have years as rows and days of the week as columns\n",
    "collisions_per_day_per_year_pivot = collisions_per_day_per_year.pivot_table(index='year', columns='day_mapped', values='NUM_COLLISIONS', fill_value=0)\n",
    "\n",
    "# Display the resulting table\n",
    "print(collisions_per_day_per_year_pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = df_cleaned[df_cleaned[\"year\"] == 2012]\n",
    "df_2012.loc[df_2012['day'] > 0, 'day'] = df_2012['day']+1 # change all days by adding 1.\n",
    "\n",
    "df_2012.loc[df_2012['day'] == 8, 'day'] = 1 # change days that equal 8 to day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the year 2013\n",
    "df_2013 = df_cleaned[df_cleaned[\"year\"] == 2013].copy()\n",
    "\n",
    "# Increment all days by 1\n",
    "df_2013.loc[df_2013['day'] > 0, 'day'] += 1\n",
    "\n",
    "# Change days that are equal to 8 to 1\n",
    "df_2013.loc[df_2013['day'] == 8, 'day'] = 1\n",
    "\n",
    "# Filter data for the year 2014\n",
    "df_2014 = df_cleaned[df_cleaned[\"year\"] == 2014].copy()\n",
    "\n",
    "# Increment all days by 1\n",
    "df_2014.loc[df_2014['day'] > 0, 'day'] += 1\n",
    "\n",
    "# Change days that are equal to 8 to 1\n",
    "df_2014.loc[df_2014['day'] == 8, 'day'] = 1\n",
    "\n",
    "# Filter data for the year 2015\n",
    "df_2015 = df_cleaned[df_cleaned[\"year\"] == 2015].copy()\n",
    "\n",
    "# Increment all days by 1\n",
    "df_2015.loc[df_2015['day'] > 0, 'day'] += 1\n",
    "\n",
    "# Change days that are equal to 8 to 1\n",
    "df_2015.loc[df_2015['day'] == 8, 'day'] = 1\n",
    "\n",
    "# Filter data for the year 2016\n",
    "df_2016 = df_cleaned[df_cleaned[\"year\"] == 2016].copy()\n",
    "\n",
    "# Increment all days by 1\n",
    "df_2016.loc[df_2016['day'] > 0, 'day'] += 1\n",
    "\n",
    "# Change days that are equal to 8 to 1\n",
    "df_2016.loc[df_2016['day'] == 8, 'day'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = [df_2013, df_2014, df_2015, df_2016]\n",
    "df_final = pd.concat(all_years)\n",
    "print(df_final[\"day\"].count())\n",
    "\n",
    "df_final.head()\n",
    "print (\"ran this!\")\n",
    "\n",
    "df_final.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out this plot\n",
    "groups = df_final.groupby('year') # We group by year as we want to create a legend and make the visualization clearer using color.\n",
    "plt.ylim(0, 1480)\n",
    "\n",
    "for name, group in groups:\n",
    "    plt.plot(group.da, group.NUM_COLLISIONS, marker='o', linestyle='', markersize=2, label=name)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Adding axis titles\n",
    "plt.xlabel('Day of the Year')\n",
    "plt.ylabel('Number of Collisions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import layers\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very simple model, with just one single input (days) and one output (NUM_COLLISIONS).\n",
    "\n",
    "one_input_data = [df_final[\"day\"], df_final[\"NUM_COLLISIONS\"]] # create an array of all values for day and all values for NUM_TRIPS in two columns\n",
    "headers = [\"day\", \"NUM_COLLISIONS\"] # declare the titles of our input and output. As you can see day is first and NUM_TRIPS is second and they correspond to the line above\n",
    "df_one_input = pd.concat(one_input_data, axis=1, keys=headers) # Bring these two arrays together to make a new dataframe\n",
    "df_one_input.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ok now to train model\n",
    " #This code is performing a common task in data science: splitting a dataset into training and test sets. \n",
    "\n",
    "train_dataset = df_one_input.sample(frac=0.8, random_state=0)\n",
    "test_dataset = df_one_input.drop(train_dataset.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_features = train_dataset.copy(): Creates a copy of the train_dataset DataFrame and stores it in train_features.\n",
    "#test_features = test_dataset.copy(): Similarly, creates a copy of the test_dataset DataFrame and stores it in test_features.\n",
    "#copy() method is used to ensure that the original train_dataset and test_dataset are not modified when changes are made to train_features and test_features.\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "#The labels (train_labels and test_labels) represent the actual number of collisions in each row of your dataset. \n",
    "# The model will use other features (input variables) to try and predict this value. \n",
    "\n",
    "train_labels = train_features.pop('NUM_COLLISIONS')\n",
    "test_labels = test_features.pop('NUM_COLLISIONS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.head())\n",
    "print(test_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels/scale_factor\n",
    "test_labels = test_labels/scale_factor\n",
    "print(train_labels)\n",
    "\n",
    "##train_labels = train_labels / scale_factor: This line divides each element in the train_labels Series by the scale_factor (which is set to 1,000,000). \n",
    "# This effectively scales down the values of the labels in the training dataset, making them smaller and potentially easier to work with.\n",
    "#test_labels = test_labels / scale_factor: \n",
    "# Similarly, this line divides each element in the test_labels Series by the same scale_factor, scaling down the labels in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_features is your training data\n",
    "\n",
    "#tf.keras.layers.Normalization: This creates a normalization layer that will standardize the features (input data).\n",
    "#axis=-1: This indicates that normalization will be applied across the last axis. \n",
    "# For example, if you have a 2D array (matrix), normalization will be applied to each row independently.\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(train_features))\n",
    "\n",
    "first = np.array(train_features[:1])\n",
    "\n",
    "# Print the first example before and after normalization\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', first)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(first).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_features is your DataFrame and 'day' is one of its columns\n",
    "day = np.array(train_features['day'])\n",
    "\n",
    "# train_features['day']: This selects the column labeled 'day' from the train_features DataFrame. \n",
    "# It assumes that train_features is a pandas DataFrame containing various features.\n",
    "\n",
    "# tf.keras.layers.Normalization: This creates a normalization layer specifically for the day feature.\n",
    "#input_shape=[1,]: This indicates that the input to the layer will be a 1D array with one element (the day value).\n",
    "#axis=None: This means that normalization will be applied to the entire input (single value), rather than along a specific axis.\n",
    "\n",
    "# Define and adapt the Normalization layer\n",
    "day_normalizer = tf.keras.layers.Normalization(input_shape=[1,], axis=None)\n",
    "day_normalizer.adapt(day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# This creates a sequential model, which is a linear stack of layers. \n",
    "# You can think of it as a chain where the output of one layer is the input to the next.\n",
    "\n",
    "# Assuming day_normalizer is already defined as in your previous code\n",
    "# Define the model\n",
    "day_model = tf.keras.Sequential([\n",
    "    day_normalizer,\n",
    "    tf.keras.layers.Dense(units=1)  # accessing Dense layer through tf.keras.layers\n",
    "])\n",
    "\n",
    "# : This adds a Dense layer with a single unit (or neuron). \n",
    "# A Dense layer is a fully connected layer where each input node is connected to each output node. \n",
    "# The output will be a single value, which might represent a prediction (for example, predicting a target variable based on the normalized day).\n",
    "\n",
    "# Print model summary\n",
    "day_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_model.predict(day[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')\n",
    "\n",
    "print(\"It worked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras model - exec training for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = day_model.fit(\n",
    "    train_features['day'],\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 0.2])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [NUM_TRIPS]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "\n",
    "test_results['day_model'] = day_model.evaluate(\n",
    "    test_features['day'],\n",
    "    test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(1, 7, 8)\n",
    "y = day_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sense check\n",
    "print(\"Unique days:\", train_features['day'].unique())\n",
    "print(\"Number of trips:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted days:\", x[:10])\n",
    "print(\"Predicted NUM_TRIPS:\", y[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(train_labels)  # Boxplot to check for outliers\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_day(x, y):\n",
    "  plt.scatter(train_features['day'], train_labels, label='Data', alpha=0.3)\n",
    "  plt.plot(x, y, color='k', label='Predictions')\n",
    "  plt.xlabel('day')\n",
    "  #plt.ylim([0.1, 0.15])\n",
    "  plt.ylabel('NUM_TRIPS')\n",
    "  plt.legend()\n",
    "\n",
    "print(x.shape) \n",
    "print(y.shape)\n",
    "\n",
    "plot_day(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
